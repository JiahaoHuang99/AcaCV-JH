
@article{yang_generative_2021,
	title = {Generative {Adversarial} {Networks} ({GAN}) {Powered} {Fast} {Magnetic} {Resonance} {Imaging} -- {Mini} {Review}, {Comparison} and {Perspectives}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2105.01800},
	abstract = {Magnetic Resonance Imaging (MRI) is a vital component of medical imaging. When compared to other image modalities, it has advantages such as the absence of radiation, superior soft tissue contrast, and complementary multiple sequence information. However, one drawback of MRI is its comparatively slow scanning and reconstruction compared to other image modalities, limiting its usage in some clinical applications when imaging time is critical. Traditional compressive sensing based MRI (CS-MRI) reconstruction can speed up MRI acquisition, but suffers from a long iterative process and noise-induced artefacts. Recently, Deep Neural Networks (DNNs) have been used in sparse MRI reconstruction models to recreate relatively high-quality images from heavily undersampled k-space data, allowing for much faster MRI scanning. However, there are still some hurdles to tackle. For example, directly training DNNs based on L1/L2 distance to the target fully sampled images could result in blurry reconstruction because L1/L2 loss can only enforce overall image or patch similarity and does not take into account local information such as anatomical sharpness. It is also hard to preserve fine image details while maintaining a natural appearance. More recently, Generative Adversarial Networks (GAN) based methods are proposed to solve fast MRI with enhanced image perceptual quality. The encoder obtains a latent space for the undersampling image, and the image is reconstructed by the decoder using the GAN loss. In this chapter, we review the GAN powered fast MRI methods with a comparative study on various anatomical datasets to demonstrate the generalisability and robustness of this kind of fast MRI while providing future perspectives.},
	author = {Yang, Guang and Lv, Jun and Chen, Yutong and Huang, Jiahao and Zhu, Jin},
	month = may,
	year = {2021},
	note = {arXiv: 2105.01800},
	file = {PDF:E\:\\Zotero\\storage\\83ANR7EP\\2105.01800.pdf:application/pdf},
}

@article{Jiang2021,
	title = {{FA}-{GAN}: {Fused} attentive generative adversarial networks for {MRI} image super-resolution},
	volume = {92},
	copyright = {All rights reserved},
	issn = {18790771},
	url = {https://doi.org/10.1016/j.compmedimag.2021.101969},
	doi = {10.1016/j.compmedimag.2021.101969},
	abstract = {High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super- resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods.},
	number = {August},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Jiang, Mingfeng and Zhi, Minghao and Wei, Liying and Yang, Xiaocheng and Zhang, Jucheng and Li, Yongming and Wang, Pin and Huang, Jiahao and Yang, Guang},
	year = {2021},
	pmid = {34411966},
	note = {Publisher: Elsevier Ltd},
	keywords = {Super-resolution, MRI, Attention, Generative adversarial networks, Mechanism},
	pages = {101969},
	file = {PDF:E\:\\Zotero\\storage\\4MEIDTK4\\1-s2.0-S089561112100118X-main.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Zotero\\storage\\RQT52UJ5\\S089561112100118X.html:text/html},
}

@article{huang_improved_2021,
	title = {Improved {YOLOv3} {Model} for miniature camera detection},
	volume = {142},
	copyright = {All rights reserved},
	issn = {00303992},
	url = {https://doi.org/10.1016/j.optlastec.2021.107133},
	doi = {10.1016/j.optlastec.2021.107133},
	abstract = {The abuse of miniature cameras has severely violated information security and privacy. Several unsolved challenges such as small or multiple targets detection as well as complex background environments in terms of active laser detection still exist, resulting in difficulties in its practical application. In this paper, an active laser detection system is proposed to obtain high-intensity cat-eye reflection images. An improved YOLOv3 model, YOLOv3-4L, was introduced to detect the actual position of the target. In the YOLOv3-4L model, each image was resized to 608×608 to preserve image details. The scales of prediction were increased from three to four, and an additional feature map was used to extract more details. YOLOv3-4L exhibited excellent performance in detecting small targets. The experimental results show that the mean average precision achieved by YOLOv3-4L was 90.37\%, compared to the 85.41\%,87.81\%, and 88.97\% achieved by traditional YOLOv3, Faster R-CNN, and Single Shot Multi-box Detector respectively. The speed of the YOLOv3-4L model meets the requirements of real-time target detection.},
	number = {March},
	journal = {Optics and Laser Technology},
	author = {Huang, Jiahao and Zhang, Haiyang and Wang, Lin and Zhang, Zilong and Zhao, Changming},
	year = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Deep learning, Computer vision, Miniature camera, Target detection, YOLOv3},
	pages = {107133},
	file = {PDF:E\:\\Zotero\\storage\\LU7Y8CU9\\1-s2.0-S0030399221002218-main.pdf:application/pdf},
}

@article{wang_tiny_2021,
	title = {Tiny camera detection technology based on hyper-spectral imaging technology},
	volume = {42},
	copyright = {All rights reserved},
	issn = {10022082},
	doi = {10.5768/JAO202142.0605001},
	abstract = {Aiming at the characteristics of hyper-spectral imaging, a scheme for detecting tiny cameras based on three-dimensional features was proposed. The results were accurately determined in the spectral dimension after the suspected target was screened out by the cat's eye effect in the space dimension. According to the camera structure, the reflection spectral characteristics of the visible light camera were analyzed. Based on the geometric optics and radiometry, the detection distance of the system was calculated and simulated. The results show that if the equipment is working properly, the optical power affects the minimum detection distance, while the target size affects the maximum detection distance. A tiny-camera spectral feature verification system was built. The results show that the non-reflective light ratio curve of the target using the absorptive infrared cut-off filter changes smoothly and the value is high. As for the curve of the target using the reflective infrared cut-off filter, the value is high in visible light part while it is low in infrared part, and the curve begins to drop near 700 nm and even mutates where the absolute slope value is more than 10 times as it in the infrared band according to the experimental data. The experimental results are consistent with the expected results of the analysis, which proves the feasibility of hyper-spectral imaging technology to detect tiny cameras.},
	number = {6},
	journal = {Journal of Applied Optics},
	author = {Wang, Lin and Zhang, Haiyang and Huang, Jiahao and Qu, Jiahui and Zhao, Changming and Zhang, Zilong},
	year = {2021},
	keywords = {Hyper-spectrum, Imaging spectrum, Spectral characteristics, Tiny camera},
	pages = {1107--1114},
	file = {PDF:E\:\\Zotero\\storage\\IICSHIUA\\Tiny+camera+detection+technology+based+on+hyper-spectral+imaging+technology.pdf:application/pdf},
}

@article{lv_transfer_2021,
	title = {Transfer learning enhanced generative adversarial networks for multi-channel {MRI} reconstruction},
	volume = {134},
	copyright = {All rights reserved},
	issn = {18790534},
	url = {https://doi.org/10.1016/j.compbiomed.2021.104504},
	doi = {10.1016/j.compbiomed.2021.104504},
	abstract = {Deep learning based generative adversarial networks (GAN) can effectively perform image reconstruction with under-sampled MR data. In general, a large number of training samples are required to improve the reconstruction performance of a certain model. However, in real clinical applications, it is difficult to obtain tens of thousands of raw patient data to train the model since saving k-space data is not in the routine clinical flow. Therefore, enhancing the generalizability of a network based on small samples is urgently needed. In this study, three novel applications were explored based on parallel imaging combined with the GAN model (PI-GAN) and transfer learning. The model was pre-trained with public Calgary brain images and then fine-tuned for use in (1) patients with tumors in our center; (2) different anatomies, including knee and liver; (3) different k-space sampling masks with acceleration factors (AFs) of 2 and 6. As for the brain tumor dataset, the transfer learning results could remove the artifacts found in PI-GAN and yield smoother brain edges. The transfer learning results for the knee and liver were superior to those of the PI-GAN model trained with its own dataset using a smaller number of training cases. However, the learning procedure converged more slowly in the knee datasets compared to the learning in the brain tumor datasets. The reconstruction performance was improved by transfer learning both in the models with AFs of 2 and 6. Of these two models, the one with AF = 2 showed better results. The results also showed that transfer learning with the pre-trained model could solve the problem of inconsistency between the training and test datasets and facilitate generalization to unseen data.},
	number = {February},
	journal = {Computers in Biology and Medicine},
	author = {Lv, Jun and Li, Guangyuan and Tong, Xiangrong and Chen, Weibo and Huang, Jiahao and Wang, Chengyan and Yang, Guang},
	year = {2021},
	pmid = {34062366},
	note = {arXiv: 2105.08175
Publisher: Elsevier Ltd},
	keywords = {Generative adversarial networks, Image reconstruction, Transfer learning, Multi-channel MRI},
	pages = {104504},
	file = {PDF:E\:\\Zotero\\storage\\288ZJ5Y6\\1-s2.0-S0010482521002985-main(2).pdf:application/pdf},
}

@misc{huang_data_2022,
	title = {Data and {Physics} {Driven} {Learning} {Models} for {Fast} {MRI} -- {Fundamentals} and {Methodologies} from {CNN}, {GAN} to {Attention} and {Transformers}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2204.01706},
	doi = {10.48550/arXiv.2204.01706},
	abstract = {Research studies have shown no qualms about using data driven deep learning models for downstream tasks in medical image analysis, e.g., anatomy segmentation and lesion detection, disease diagnosis and prognosis, and treatment planning. However, deep learning models are not the sovereign remedy for medical image analysis when the upstream imaging is not being conducted properly (with artefacts). This has been manifested in MRI studies, where the scanning is typically slow, prone to motion artefacts, with a relatively low signal to noise ratio, and poor spatial and/or temporal resolution. Recent studies have witnessed substantial growth in the development of deep learning techniques for propelling fast MRI. This article aims to (1) introduce the deep learning based data driven techniques for fast MRI including convolutional neural network and generative adversarial network based methods, (2) survey the attention and transformer based models for speeding up MRI reconstruction, and (3) detail the research in coupling physics and data driven models for MRI acceleration. Finally, we will demonstrate through a few clinical applications, explain the importance of data harmonisation and explainable models for such fast MRI techniques in multicentre and multi-scanner studies, and discuss common pitfalls in current research and recommendations for future research directions.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Huang, Jiahao and Fang, Yingying and Nan, Yang and Wu, Huanjun and Wu, Yinzhe and Gao, Zhifan and Li, Yang and Wang, Zidong and Lio, Pietro and Rueckert, Daniel and Eldar, Yonina C. and Yang, Guang},
	month = apr,
	year = {2022},
	note = {arXiv:2204.01706 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\AMDYG2N4\\Huang 等。 - 2022 - Data and Physics Driven Learning Models for Fast M.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\74Y6T3H3\\2204.html:text/html},
}

@inproceedings{huang_fast_2022,
	title = {Fast {MRI} {Reconstruction}: {How} {Powerful} {Transformers} {Are}?},
	copyright = {All rights reserved},
	shorttitle = {Fast {MRI} {Reconstruction}},
	doi = {10.1109/EMBC48229.2022.9871475},
	abstract = {Magnetic resonance imaging (MRI) is a widely used non-radiative and non-invasive method for clinical interro-gation of organ structures and metabolism, with an inherently long scanning time. Methods by k-space undersampling and deep learning based reconstruction have been popularised to accelerate the scanning process. This work focuses on investigating how powerful transformers are for fast MRI by exploiting and comparing different novel network architectures. In particular, a generative adversarial network (GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI reconstruction. To further preserve the edge and texture information, edge enhanced GAN based Swin transformer (EES-GAN) and texture enhanced GAN based Swin transformer (TES-GAN) were also developed, where a dual-discriminator GAN structure was applied. We compared our proposed GAN based transformers, standalone Swin transformer and other convolutional neural networks based GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that transformers work well for the MRI reconstruction from different undersampling conditions. The utilisation of GAN's adversarial structure improves the quality of images reconstructed when undersampled for 30\% or higher. The code is publicly available at https://github.comJayanglab/SwinGANMR.},
	booktitle = {2022 44th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	author = {Huang, Jiahao and Wu, Yinzhe and Wu, Huanjun and Yang, Guang},
	month = jul,
	year = {2022},
	note = {ISSN: 2694-0604},
	keywords = {Magnetic resonance imaging, Generative adversarial networks, Transformers, Visualization, Measurement, Biological system modeling, Network architecture},
	pages = {2066--2070},
	file = {IEEE Xplore Abstract Record:E\:\\Zotero\\storage\\D4VS25JW\\9871475.html:text/html;IEEE Xplore Full Text PDF:E\:\\Zotero\\storage\\RCA54EIN\\Huang et al. - 2022 - Fast MRI Reconstruction How Powerful Transformers.pdf:application/pdf},
}

@inproceedings{huang_swin_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Swin {Deformable} {Attention} {U}-{Net} {Transformer} ({SDAUT}) for {Explainable} {Fast} {MRI}},
	copyright = {All rights reserved},
	isbn = {978-3-031-16446-0},
	doi = {10.1007/978-3-031-16446-0_51},
	abstract = {Fast MRI aims to reconstruct a high fidelity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Huang, Jiahao and Xing, Xiaodan and Gao, Zhifan and Yang, Guang},
	editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
	year = {2022},
	keywords = {Transformer, Fast MRI, XAI},
	pages = {538--548},
	file = {Full Text PDF:E\:\\Zotero\\storage\\E3LZQ6VN\\Huang et al. - 2022 - Swin Deformable Attention U-Net Transformer (SDAUT.pdf:application/pdf},
}

@article{huang_edge-enhanced_2022,
	title = {Edge-enhanced dual discriminator generative adversarial network for fast {MRI} with parallel imaging using multi-view information},
	volume = {52},
	copyright = {All rights reserved},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-021-03092-w},
	doi = {10.1007/s10489-021-03092-w},
	abstract = {In clinical medicine, magnetic resonance imaging (MRI) is one of the most important tools for diagnosis, triage, prognosis, and treatment planning. However, MRI suffers from an inherent slow data acquisition process because data is collected sequentially in k-space. In recent years, most MRI reconstruction methods proposed in the literature focus on holistic image reconstruction rather than enhancing the edge information. This work steps aside this general trend by elaborating on the enhancement of edge information. Specifically, we introduce a novel parallel imaging coupled dual discriminator generative adversarial network (PIDD-GAN) for fast multi-channel MRI reconstruction by incorporating multi-view information. The dual discriminator design aims to improve the edge information in MRI reconstruction. One discriminator is used for holistic image reconstruction, whereas the other one is responsible for enhancing edge information. An improved U-Net with local and global residual learning is proposed for the generator. Frequency channel attention blocks (FCA Blocks) are embedded in the generator for incorporating attention mechanisms. Content loss is introduced to train the generator for better reconstruction quality. We performed comprehensive experiments on Calgary-Campinas public brain MR dataset and compared our method with state-of-the-art MRI reconstruction methods. Ablation studies of residual learning were conducted on the MICCAI13 dataset to validate the proposed modules. Results show that our PIDD-GAN provides high-quality reconstructed MR images, with well-preserved edge information. The time of single-image reconstruction is below 5ms, which meets the demand of faster processing.},
	language = {en},
	number = {13},
	urldate = {2022-10-07},
	journal = {Applied Intelligence},
	author = {Huang, Jiahao and Ding, Weiping and Lv, Jun and Yang, Jingwen and Dong, Hao and Del Ser, Javier and Xia, Jun and Ren, Tiaojuan and Wong, Stephen T. and Yang, Guang},
	month = jan,
	year = {2022},
	keywords = {parallel imaging, Generative adversarial networks, fast mri, generative adversarial networks, Multi-view learning, Parallel imaging, edge enhancement, Edge enhancement, Fast MRI, multi-view learning},
	pages = {14693--14710},
	file = {Full Text PDF:E\:\\Zotero\\storage\\44EN2HP6\\Huang et al. - 2022 - Edge-enhanced dual discriminator generative advers.pdf:application/pdf},
}

@article{huang_swin_2022-1,
	title = {Swin transformer for fast {MRI}},
	volume = {493},
	copyright = {All rights reserved},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222004179},
	doi = {10.1016/j.neucom.2022.04.051},
	abstract = {Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced SwinMR, a novel Swin transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual Swin transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of Swin transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our SwinMR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/SwinMR.},
	language = {en},
	urldate = {2022-10-07},
	journal = {Neurocomputing},
	author = {Huang, Jiahao and Fang, Yingying and Wu, Yinzhe and Wu, Huanjun and Gao, Zhifan and Li, Yang and Ser, Javier Del and Xia, Jun and Yang, Guang},
	month = jul,
	year = {2022},
	keywords = {MRI reconstruction, Compressed sensing, Transformer, Parallel imaging},
	pages = {281--304},
	file = {ScienceDirect Full Text PDF:E\:\\Zotero\\storage\\GPC7LUPB\\Huang et al. - 2022 - Swin transformer for fast MRI.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Zotero\\storage\\XQV9EGEY\\S0925231222004179.html:text/html},
}

@incollection{yang_generative_2022,
	address = {Cham},
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {Generative {Adversarial} {Network} {Powered} {Fast} {Magnetic} {Resonance} {Imaging}—{Comparative} {Study} and {New} {Perspectives}},
	copyright = {All rights reserved},
	isbn = {978-3-030-91390-8},
	url = {https://doi.org/10.1007/978-3-030-91390-8_13},
	abstract = {Magnetic Resonance Imaging (MRI) is a vital component of medical imaging. When compared to other image modalities, it has advantages such as the absence of radiation, superior soft tissue contrast, and complementary multiple sequence information. However, one drawback of MRI is its comparatively slow scanning and reconstruction compared to other image modalities, limiting its usage in some clinical applications when imaging time is critical. Traditional compressive sensing based MRI (CS-MRI) reconstruction can speed up MRI acquisition, but suffers from a long iterative process and noise-induced artefacts. Recently, Deep Neural Networks (DNNs) have been used in sparse MRI reconstruction models to recreate relatively high-quality images from heavily undersampled k-space data, allowing for much faster MRI scanning. However, there are still some hurdles to tackle. For example, directly training DNNs based on L1/L2 distance to the target fully sampled images could result in blurry reconstruction because L1/L2 loss can only enforce overall image or patch similarity and does not take into account local information such as anatomical sharpness. It is also hard to preserve fine image details while maintaining a natural appearance. More recently, Generative Adversarial Networks (GAN) based methods are proposed to solve fast MRI with enhanced image perceptual quality. The encoder obtains a latent space for the undersampling image, and the image is reconstructed by the decoder using the GAN loss. In this chapter, we review the GAN powered fast MRI methods with a comparative study on various anatomical datasets to demonstrate the generalisability and robustness of this kind of fast MRI while providing future perspectives.},
	language = {en},
	urldate = {2022-10-07},
	booktitle = {Generative {Adversarial} {Learning}: {Architectures} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Yang, Guang and Lv, Jun and Chen, Yutong and Huang, Jiahao and Zhu, Jin},
	editor = {Razavi-Far, Roozbeh and Ruiz-Garcia, Ariel and Palade, Vasile and Schmidhuber, Juergen},
	year = {2022},
	doi = {10.1007/978-3-030-91390-8_13},
	keywords = {Deep learning, Compressive sensing, Fast magnetic resonance imaging (mri), Generative adversarial networks (gan)},
	pages = {305--339},
	file = {Full Text PDF:E\:\\Zotero\\storage\\EQPIXJXT\\Yang et al. - 2022 - Generative Adversarial Network Powered Fast Magnet.pdf:application/pdf},
}

@inproceedings{xing_cs2_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CS}\$\${\textasciicircum}2\$\$: {A} {Controllable} and {Simultaneous} {Synthesizer} of {Images} and {Annotations} with {Minimal} {Human} {Intervention}},
	copyright = {All rights reserved},
	isbn = {978-3-031-16452-1},
	shorttitle = {{CS}\$\${\textasciicircum}2\$\$},
	doi = {10.1007/978-3-031-16452-1_1},
	abstract = {The destitution of image data and corresponding expert annotations limit the training capacities of AI diagnostic models and potentially inhibit their performance. To address such a problem of data and label scarcity, generative models have been developed to augment the training datasets. Previously proposed generative models usually require manually adjusted annotations (e.g., segmentation masks) or need pre-labeling. However, studies have found that these pre-labeling based methods can induce hallucinating artifacts, which might mislead the downstream clinical tasks, while manual adjustment could be onerous and subjective. To avoid manual adjustment and pre-labeling, we propose a novel controllable and simultaneous synthesizer (dubbed CS\$\${\textasciicircum}2\$\$) in this study to generate both realistic images and corresponding annotations at the same time. Our CS\$\${\textasciicircum}2\$\$model is trained and validated using high resolution CT (HRCT) data collected from COVID-19 patients to realize an efficient infections segmentation with minimal human intervention. Our contributions include 1) a conditional image synthesis network that receives both style information from reference CT images and structural information from unsupervised segmentation masks, and 2) a corresponding segmentation mask synthesis network to automatically segment these synthesized images simultaneously. Our experimental studies on HRCT scans collected from COVID-19 patients demonstrate that our CS\$\${\textasciicircum}2\$\$model can lead to realistic synthesized datasets and promising segmentation results of COVID infections compared to the state-of-the-art nnUNet trained and fine-tuned in a fully supervised manner.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Xing, Xiaodan and Huang, Jiahao and Nan, Yang and Wu, Yinzhe and Wang, Chengjia and Gao, Zhifan and Walsh, Simon and Yang, Guang},
	editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
	year = {2022},
	keywords = {Generative model, Semi-supervised segmentation, Data augmentation},
	pages = {3--12},
	file = {Full Text PDF:E\:\\Zotero\\storage\\526RU654\\Xing 等 - 2022 - CS\$\$^2\$\$ A Controllable and Simultaneous Synthesi.pdf:application/pdf},
}

@inproceedings{huang_vigu_2023,
	title = {{ViGU}: {Vision} {GNN} {U}-{Net} for fast {MRI}},
	copyright = {All rights reserved},
	shorttitle = {{ViGU}},
	url = {https://ieeexplore.ieee.org/abstract/document/10230600},
	doi = {10.1109/ISBI53787.2023.10230600},
	abstract = {Deep learning models have been widely applied for fast MRI. The majority of existing deep learning models, e.g., convolutional neural networks, work on data with Euclidean or regular grids structures. However, high-dimensional features extracted from MR data could be encapsulated in non-Euclidean manifolds. This disparity between the go-to assumption of existing models and data requirements limits the flexibility to capture irregular anatomical features in MR data. In this work, we introduce a novel Vision GNN type network for fast MRI called Vision GNN U-Net (ViGU). More precisely, the pixel array is first embedded into patches and then converted into a graph. Secondly, a U-shape network is developed using several graph blocks in symmetrical encoder and decoder paths. Moreover, we show that the proposed ViGU can also benefit from Generative Adversarial Networks yielding to its variant ViGU-GAN. We demonstrate, through numerical and visual experiments, that the proposed ViGU and GAN variant outperform existing CNN and GAN-based methods. Moreover, we show that the proposed network readily competes with approaches based on Transformers while requiring a fraction of the computational cost. More importantly, the graph structure of the network reveals how the network extracts features from MR images, providing intuitive explainability. The code is publicly available at https://github.com/ayanglab/ViGU.},
	urldate = {2024-02-02},
	booktitle = {2023 {IEEE} 20th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Huang, Jiahao and Aviles-Rivero, Angelica I. and Schönlieb, Carola-Bibiane and Yang, Guang},
	month = apr,
	year = {2023},
	note = {ISSN: 1945-8452},
	keywords = {Deep learning, Neural networks, Magnetic resonance imaging, Generative adversarial networks, Feature extraction, Fast MRI, Visualization, Manifolds, Graph Neural Network (GNN)},
	pages = {1--5},
	file = {IEEE Xplore Full Text PDF:E\:\\Zotero\\storage\\VMSCKGAT\\Huang 等 - 2023 - ViGU Vision GNN U-Net for fast MRI.pdf:application/pdf},
}

@inproceedings{huang_cdiffmr_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CDiffMR}: {Can} {We} {Replace} the {Gaussian} {Noise} with {K}-{Space} {Undersampling} for {Fast} {MRI}?},
	copyright = {All rights reserved},
	isbn = {978-3-031-43999-5},
	shorttitle = {{CDiffMR}},
	doi = {10.1007/978-3-031-43999-5_1},
	abstract = {Deep learning has shown the capability to substantially accelerate MRI reconstruction while acquiring fewer measurements. Recently, diffusion models have gained burgeoning interests as a novel group of deep learning-based generative methods. These methods seek to sample data points that belong to a target distribution from a Gaussian distribution, which has been successfully extended to MRI reconstruction. In this work, we proposed a Cold Diffusion-based MRI reconstruction method called CDiffMR. Different from conventional diffusion models, the degradation operation of our CDiffMR is based on k-space undersampling instead of adding Gaussian noise, and the restoration network is trained to harness a de-aliaseing function. We also design starting point and data consistency conditioning strategies to guide and accelerate the reverse process. More intriguingly, the pre-trained CDiffMR model can be reused for reconstruction tasks with different undersampling rates. We demonstrated, through extensive numerical and visual experiments, that the proposed CDiffMR can achieve comparable or even superior reconstruction results than state-of-the-art models. Compared to the diffusion model-based counterpart, CDiffMR reaches readily competing results using only 1.6–3.4\% for inference time. The code is publicly available at https://github.com/ayanglab/CDiffMR.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Huang, Jiahao and Aviles-Rivero, Angelica I. and Schönlieb, Carola-Bibiane and Yang, Guang},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	keywords = {Deep Learning, Fast MRI, Diffusion Models},
	pages = {3--12},
	file = {已提交版本:E\:\\Zotero\\storage\\YJGJH9TD\\Huang 等 - 2023 - CDiffMR Can We Replace the Gaussian Noise with K-.pdf:application/pdf},
}

@inproceedings{wang_style_2023,
	title = {Style {Transfer} and {Self}-{Supervised} {Learning} {Powered} {Myocardium} {Infarction} {Super}-{Resolution} {Segmentation}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/10373454},
	doi = {10.1109/SIPAIM56729.2023.10373454},
	abstract = {This study proposes a pipeline that incorporates a novel style transfer model and a simultaneous super-resolution and segmentation model. The proposed pipeline aims to enhance diffusion tensor imaging (DTI) images by translating them into the late gadolinium enhancement (LGE) domain, which offers a larger amount of data with high-resolution and distinct highlighting of myocardium infarction (MI) areas. Subsequently, the segmentation task is performed on the LGE style image. An end-to-end super-resolution segmentation model is introduced to generate high-resolution mask from low-resolution LGE style DTI image. Further, to enhance the performance of the model, a multi-task self-supervised learning strategy is employed to pre-train the super-resolution segmentation model, allowing it to acquire more representative knowledge and improve its segmentation performance after fine-tuning. https://github.com/wlc2424762917/Med\_Img},
	urldate = {2024-02-02},
	booktitle = {2023 19th {International} {Symposium} on {Medical} {Information} {Processing} and {Analysis} ({SIPAIM})},
	author = {Wang, Lichao and Huang, Jiahao and Xing, Xiaodan and Wu, Yinzhe and Rajakulasingam, Ramyah and Scott, Andrew D. and Ferreira, Pedro F and Silva, Ranil De and Nielles-Vallespin, Sonia and Yang, Guang},
	month = nov,
	year = {2023},
	keywords = {self-supervised learning, Superresolution, Image segmentation, Diffusion tensor imaging, Self-supervised learning, Transforms, Pipelines, late gadolinium enhancement, Myocardium, myocardium infarction segmentation, style transfer, Tensors},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:E\:\\Zotero\\storage\\RLTKIS86\\10373454.html:text/html;IEEE Xplore Full Text PDF:E\:\\Zotero\\storage\\ISWYYLYD\\Wang 等 - 2023 - Style Transfer and Self-Supervised Learning Powere.pdf:application/pdf},
}

@inproceedings{ordonez_beyond_2023,
	title = {Beyond {U}: {Making} {Diffusion} {Models} {Faster} \& {Lighter}},
	copyright = {All rights reserved},
	shorttitle = {Beyond {U}},
	url = {https://openreview.net/forum?id=fVndxbNUmt},
	abstract = {Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70\% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutions.},
	language = {en},
	urldate = {2024-02-02},
	author = {Ordoñez, Sergio Calvo and Huang, Jiahao and Zhang, Lipei and Yang, Guang and Schönlieb, Carola-Bibiane and Aviles-Rivero, Angelica},
	month = sep,
	year = {2023},
	keywords = {},
	file = {Full Text PDF:E\:\\Zotero\\storage\\DK4AV8PU\\Ordoñez 等 - 2023 - Beyond U Making Diffusion Models Faster & Lighter.pdf:application/pdf},
}

@misc{wang_efficient_2024,
	title = {Efficient {Post}-processing of {Diffusion} {Tensor} {Cardiac} {Magnetic} {Imaging} {Using} {Texture}-conserving {Deformable} {Registration}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2309.06598},
	doi = {10.48550/arXiv.2309.06598},
	abstract = {Diffusion tensor cardiac magnetic resonance (DT-CMR) is a method capable of providing non-invasive measurements of myocardial microstructure. Image registration is essential to correct image shifts due to intra and inter breath-hold motion and imperfect cardiac triggering. Registration is challenging in DT-CMR due to the low signal-to-noise and various contrasts induced by the diffusion encoding in the myocardium and surrounding organs. Traditional deformable registration corrects through-plane motion but at the risk of destroying the texture information while rigid registration inefficiently discards frames with local deformation. In this study, we explored the possibility of deep learning-based deformable registration on DT-CMR. Based on the noise suppression using low-rank features and diffusion encoding suppression using variational auto encoder-decoder, a B-spline based registration network extracted the displacement fields and maintained the texture features of DT-CMR. In this way, our method improved the efficiency of frame utilization, manual cropping, and computational speed.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Wang, Fanwen and Ferreira, Pedro F. and Wu, Yinzhe and Munoz, Camila and Wen, Ke and Luo, Yaqing and Huang, Jiahao and Pennell, Dudley J. and Scott, Andrew D. and Nielles-Vallespin, Sonia and Yang, Guang},
	month = jan,
	year = {2024},
	note = {arXiv:2309.06598 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\4FKFQ274\\Wang 等 - 2024 - Efficient Post-processing of Diffusion Tensor Card.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\MMDJR58E\\2309.html:text/html},
}

@inproceedings{wang_hybrid_2023,
	title = {Hybrid {Swin} {Deformable} {Attention} {U}-{Net} for {Medical} {Image} {Segmentation}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/10373513},
	doi = {10.1109/SIPAIM56729.2023.10373513},
	abstract = {Medical image segmentation is a crucial task in the field of medical image analysis. Harmonizing the convolution and multi-head self-attention mechanism is a recent research focus in this field, with various combination methods proposed. However, the lack of interpretability of these hybrid models remains a common pitfall, limiting their practical application in clinical scenarios. To address this issue, we propose to incorporate the Shifted Window (Swin) Deformable Attention into a hybrid architecture to improve segmentation performance while ensuring explainability. Our proposed Swin Deformable Attention Hybrid UNet (SDAH-UNet) demonstrates state-of-the-art performance on both anatomical and lesion segmentation tasks. Moreover, we provide a direct and visual explanation of the model focalization and how the model forms it, enabling clinicians to better understand and trust the decision of the model. Our approach could be a promising solution to the challenge of developing accurate and interpretable medical image segmentation models. https://github.com/wlc2424762917/SDAH\_UNet},
	urldate = {2024-02-02},
	booktitle = {2023 19th {International} {Symposium} on {Medical} {Information} {Processing} and {Analysis} ({SIPAIM})},
	author = {Wang, Lichao and Huang, Jiahao and Xing, Xiaodan and Yang, Guang},
	month = nov,
	year = {2023},
	keywords = {Computational modeling, Medical image segmentation, transformer, Image segmentation, Visualization, Convolution, XAI, Deformable models, deformable attention, Limiting, Redundancy},
	pages = {1--5},
	file = {IEEE Xplore Full Text PDF:E\:\\Zotero\\storage\\KVU9WUUQ\\Wang 等 - 2023 - Hybrid Swin Deformable Attention U-Net for Medical.pdf:application/pdf},
}

@misc{huang_deep_2023,
	title = {Deep {Learning}-based {Diffusion} {Tensor} {Cardiac} {Magnetic} {Resonance} {Reconstruction}: {A} {Comparison} {Study}},
	copyright = {All rights reserved},
	shorttitle = {Deep {Learning}-based {Diffusion} {Tensor} {Cardiac} {Magnetic} {Resonance} {Reconstruction}},
	url = {http://arxiv.org/abs/2304.00996},
	doi = {10.48550/arXiv.2304.00996},
	abstract = {In vivo cardiac diffusion tensor imaging (cDTI) is a promising Magnetic Resonance Imaging (MRI) technique for evaluating the micro-structure of myocardial tissue in the living heart, providing insights into cardiac function and enabling the development of innovative therapeutic strategies. However, the integration of cDTI into routine clinical practice is challenging due to the technical obstacles involved in the acquisition, such as low signal-to-noise ratio and long scanning times. In this paper, we investigate and implement three different types of deep learning-based MRI reconstruction models for cDTI reconstruction. We evaluate the performance of these models based on reconstruction quality assessment and diffusion tensor parameter assessment. Our results indicate that the models we discussed in this study can be applied for clinical use at an acceleration factor (AF) of \${\textbackslash}times 2\$ and \${\textbackslash}times 4\$, with the D5C5 model showing superior fidelity for reconstruction and the SwinMR model providing higher perceptual scores. There is no statistical difference with the reference for all diffusion tensor parameters at AF \${\textbackslash}times 2\$ or most DT parameters at AF \${\textbackslash}times 4\$, and the quality of most diffusion tensor parameter maps are visually acceptable. SwinMR is recommended as the optimal approach for reconstruction at AF \${\textbackslash}times 2\$ and AF \${\textbackslash}times 4\$. However, we believed the models discussed in this studies are not prepared for clinical use at a higher AF. At AF \${\textbackslash}times 8\$, the performance of all models discussed remains limited, with only half of the diffusion tensor parameters being recovered to a level with no statistical difference from the reference. Some diffusion tensor parameter maps even provide wrong and misleading information.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Huang, Jiahao and Ferreira, Pedro F. and Wang, Lichao and Wu, Yinzhe and Aviles-Rivero, Angelica I. and Schonlieb, Carola-Bibiane and Scott, Andrew D. and Khalique, Zohya and Dwornik, Maria and Rajakulasingam, Ramyah and De Silva, Ranil and Pennell, Dudley J. and Nielles-Vallespin, Sonia and Yang, Guang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00996 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\R4IJWCD9\\Huang 等 - 2023 - Deep Learning-based Diffusion Tensor Cardiac Magne.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\UDXLGHCK\\2304.html:text/html},
}

@misc{wu_high-resolution_2023,
	title = {High-{Resolution} {Reference} {Image} {Assisted} {Volumetric} {Super}-{Resolution} of {Cardiac} {Diffusion} {Weighted} {Imaging}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2310.20389},
	doi = {10.48550/arXiv.2310.20389},
	abstract = {Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is the only in vivo method to non-invasively examine the microstructure of the human heart. Current research in DT-CMR aims to improve the understanding of how the cardiac microstructure relates to the macroscopic function of the healthy heart as well as how microstructural dysfunction contributes to disease. To get the final DT-CMR metrics, we need to acquire diffusion weighted images of at least 6 directions. However, due to DWI's low signal-to-noise ratio, the standard voxel size is quite big on the scale for microstructures. In this study, we explored the potential of deep-learning-based methods in improving the image quality volumetrically (x4 in all dimensions). This study proposed a novel framework to enable volumetric super-resolution, with an additional model input of high-resolution b0 DWI. We demonstrated that the additional input could offer higher super-resolved image quality. Going beyond, the model is also able to super-resolve DWIs of unseen b-values, proving the model framework's generalizability for cardiac DWI superresolution. In conclusion, we would then recommend giving the model a high-resolution reference image as an additional input to the low-resolution image for training and inference to guide all super-resolution frameworks for parametric imaging where a reference image is available.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Wu, Yinzhe and Huang, Jiahao and Wang, Fanwen and Ferreira, Pedro and Scott, Andrew and Nielles-Vallespin, Sonia and Yang, Guang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20389 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\X2544WXT\\Wu 等 - 2023 - High-Resolution Reference Image Assisted Volumetri.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\9LB8Y5QY\\2310.html:text/html},
}

@misc{huang_data_2024,
	title = {Data and {Physics} driven {Deep} {Learning} {Models} for {Fast} {MRI} {Reconstruction}: {Fundamentals} and {Methodologies}},
	copyright = {All rights reserved},
	shorttitle = {Data and {Physics} driven {Deep} {Learning} {Models} for {Fast} {MRI} {Reconstruction}},
	url = {http://arxiv.org/abs/2401.16564},
	doi = {10.48550/arXiv.2401.16564},
	abstract = {Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based models, and plug-and-play models to emergent full spectrum of generative models. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, emphasizing the role of data harmonization, and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Huang, Jiahao and Wu, Yinzhe and Wang, Fanwen and Fang, Yingying and Nan, Yang and Alkan, Cagan and Xu, Lei and Gao, Zhifan and Wu, Weiwen and Zhu, Lei and Chen, Zhaolin and Lally, Peter and Bangerter, Neal and Setsompop, Kawin and Guo, Yike and Rueckert, Daniel and Wang, Ge and Yang, Guang},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16564 [eess]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\ERH5GUIQ\\Huang 等 - 2024 - Data and Physics driven Deep Learning Models for F.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\8XUVBWI8\\2401.html:text/html},
}

@misc{bryutkin_hamlet_2024,
	title = {{HAMLET}: {Graph} {Transformer} {Neural} {Operator} for {Partial} {Differential} {Equations}},
	copyright = {All rights reserved},
	shorttitle = {{HAMLET}},
	url = {http://arxiv.org/abs/2402.03541},
	doi = {10.48550/arXiv.2402.03541},
	abstract = {We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Bryutkin, Andrey and Huang, Jiahao and Deng, Zhongying and Yang, Guang and Schönlieb, Carola-Bibiane and Aviles-Rivero, Angelica},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03541 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:E\:\\Zotero\\storage\\82IBGHJC\\Bryutkin 等 - 2024 - HAMLET Graph Transformer Neural Operator for Part.pdf:application/pdf;arXiv.org Snapshot:E\:\\Zotero\\storage\\DK9KG4II\\2402.html:text/html},
}
